Step 1, Loss: -2.166760193747903  gradient: 0.2560254250190814
Step 11, Loss: -2.1182675447917227  gradient: 2.3018062708008973
Step 21, Loss: -2.1575654931115453  gradient: 0.7125803455343254
Step 31, Loss: -2.1605793325632927  gradient: 0.4501145146010253
Step 41, Loss: -2.1635062517021555  gradient: 0.2722550428816826
Step 51, Loss: -2.165591117524955  gradient: 0.1363712460863964
Step 61, Loss: -2.1665225063317566  gradient: 0.09523681435731193
Step 71, Loss: -2.167444565111686  gradient: 0.06247140397302932
Step 81, Loss: -2.1684390625639445  gradient: 0.048723556172723274
Step 91, Loss: -2.1694977807620632  gradient: 0.030317037394716292
Step 101, Loss: -2.1707169155720645  gradient: 0.025707562140415273
Step 111, Loss: -2.1722901138443422  gradient: 0.025969632361959975
Step 121, Loss: -2.1746313065609586  gradient: 0.027695864984181494
Step 131, Loss: -2.17876404314887  gradient: 0.04005759561595611
Step 141, Loss: -2.186367511601275  gradient: 0.04472475900589428
Step 151, Loss: -2.193959655147629  gradient: 0.037719570538155285
Step 161, Loss: -2.1967372282404267  gradient: 0.0327658415620494
Step 171, Loss: -2.198510133147186  gradient: 0.01683921287628664
Step 181, Loss: -2.1992151193250176  gradient: 0.009019080349457262
Step 191, Loss: -2.1993668407511255  gradient: 0.005406511823260885
Step 201, Loss: -2.1994099245345677  gradient: 0.0030278576617645304
Step 211, Loss: -2.199423912711982  gradient: 0.0018382032872150246
convergent at epoch 220; gradient 0.0009538325020612614 is below threshold