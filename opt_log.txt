Step 1, Loss: -2.166760193747895  gradient: 0.25602542501905934
Step 11, Loss: -2.118267544805733  gradient: 2.3018062691141368
Step 21, Loss: -2.157565493116625  gradient: 0.7125803455809645
Step 31, Loss: -2.160579332554267  gradient: 0.4501145149866964
Step 41, Loss: -2.1635062516948182  gradient: 0.27225504309085535
Step 51, Loss: -2.165591117520422  gradient: 0.13637124600387635
Step 61, Loss: -2.166522506328189  gradient: 0.0952368144280118
Step 71, Loss: -2.1674445651098537  gradient: 0.0624714040595549
Step 81, Loss: -2.16843906256587  gradient: 0.04872355628760218
Step 91, Loss: -2.169497780771488  gradient: 0.03031703742593716
Step 101, Loss: -2.1707169155985686  gradient: 0.02570756242131493
Step 111, Loss: -2.1722901139149076  gradient: 0.02596963298799277
Step 121, Loss: -2.1746313067572576  gradient: 0.027695866412290323
Step 131, Loss: -2.1787640437000153  gradient: 0.040057597792069265
Step 141, Loss: -2.1863675125682986  gradient: 0.04472475822687744
Step 151, Loss: -2.1939596552730674  gradient: 0.03771956820744394
Step 161, Loss: -2.1967372280595456  gradient: 0.03276584050599004
Step 171, Loss: -2.1985101330761854  gradient: 0.016839212204844273
Step 181, Loss: -2.1992151192941494  gradient: 0.009019080378609066
Step 191, Loss: -2.199366840745745  gradient: 0.005406511870421399
Step 201, Loss: -2.199409924535965  gradient: 0.003027857491303753
Step 211, Loss: -2.1994239127112047  gradient: 0.0018382033717539098
convergent at epoch 220; gradient 0.0009538324002433562 is below threshold