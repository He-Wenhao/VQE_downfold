{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hewenhao/anaconda3/envs/ML_DFT/lib/python3.8/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/hewenhao/anaconda3/envs/ML_DFT/lib/python3.8/site-packages/torch/cuda/__init__.py:749: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, x: [1.100000023841858, -1.100000023841858], Loss: 20.0\n",
      "Step 2, x: [1.1998335123062134, -1.1999260187149048], Loss: 18.82000160217285\n",
      "Step 3, x: [1.2993766069412231, -1.299726128578186], Loss: 17.681161880493164\n",
      "Step 4, x: [1.398495078086853, -1.3993467092514038], Loss: 16.584148406982422\n",
      "Step 5, x: [1.4970442056655884, -1.4987324476242065], Loss: 15.529521942138672\n",
      "Step 6, x: [1.5948683023452759, -1.5978261232376099], Loss: 14.517749786376953\n",
      "Step 7, x: [1.691800594329834, -1.6965692043304443], Loss: 13.549182891845703\n",
      "Step 8, x: [1.7876631021499634, -1.794901728630066], Loss: 12.624040603637695\n",
      "Step 9, x: [1.8822671175003052, -1.892762303352356], Loss: 11.742415428161621\n",
      "Step 10, x: [1.975413203239441, -1.9900883436203003], Loss: 10.904253959655762\n",
      "Step 11, x: [2.06689190864563, -2.0868163108825684], Loss: 10.109345436096191\n",
      "Step 12, x: [2.156484842300415, -2.1828815937042236], Loss: 9.357329368591309\n",
      "Step 13, x: [2.2439656257629395, -2.2782187461853027], Loss: 8.647674560546875\n",
      "Step 14, x: [2.329102039337158, -2.3727622032165527], Loss: 7.979681015014648\n",
      "Step 15, x: [2.4116580486297607, -2.4664454460144043], Loss: 7.352482795715332\n",
      "Step 16, x: [2.491396188735962, -2.559201955795288], Loss: 6.765044689178467\n",
      "Step 17, x: [2.568080425262451, -2.6509649753570557], Loss: 6.21617317199707\n",
      "Step 18, x: [2.641479730606079, -2.7416679859161377], Loss: 5.704519748687744\n",
      "Step 19, x: [2.711372137069702, -2.831244707107544], Loss: 5.228600025177002\n",
      "Step 20, x: [2.777547836303711, -2.9196295738220215], Loss: 4.786805152893066\n",
      "Step 21, x: [2.8398141860961914, -3.0067574977874756], Loss: 4.377425670623779\n",
      "Step 22, x: [2.8979992866516113, -3.092564582824707], Loss: 3.9986753463745117\n",
      "Step 23, x: [2.951955795288086, -3.176987648010254], Loss: 3.648714065551758\n",
      "Step 24, x: [3.0015645027160645, -3.259965419769287], Loss: 3.3256821632385254\n",
      "Step 25, x: [3.0467371940612793, -3.341437816619873], Loss: 3.0277228355407715\n",
      "Step 26, x: [3.087419271469116, -3.421346664428711], Loss: 2.7530128955841064\n",
      "Step 27, x: [3.1235909461975098, -3.499635934829712], Loss: 2.499788284301758\n",
      "Step 28, x: [3.1552681922912598, -3.57625150680542], Loss: 2.266367197036743\n",
      "Step 29, x: [3.1825027465820312, -3.6511423587799072], Loss: 2.0511679649353027\n",
      "Step 30, x: [3.205380916595459, -3.724259614944458], Loss: 1.8527240753173828\n",
      "Step 31, x: [3.224022150039673, -3.7955574989318848], Loss: 1.6696949005126953\n",
      "Step 32, x: [3.238577127456665, -3.864993095397949], Loss: 1.5008676052093506\n",
      "Step 33, x: [3.2492246627807617, -3.9325270652770996], Loss: 1.3451597690582275\n",
      "Step 34, x: [3.2561686038970947, -3.9981234073638916], Loss: 1.2016113996505737\n",
      "Step 35, x: [3.2596347332000732, -4.0617499351501465], Loss: 1.0693790912628174\n",
      "Step 36, x: [3.259866952896118, -4.123377799987793], Loss: 0.9477233290672302\n",
      "Step 37, x: [3.257124185562134, -4.182982444763184], Loss: 0.8359972834587097\n",
      "Step 38, x: [3.251676559448242, -4.240542888641357], Loss: 0.7336305379867554\n",
      "Step 39, x: [3.243802070617676, -4.2960429191589355], Loss: 0.6401162147521973\n",
      "Step 40, x: [3.233783721923828, -4.3494696617126465], Loss: 0.554995059967041\n",
      "Step 41, x: [3.2219061851501465, -4.400815486907959], Loss: 0.47784456610679626\n",
      "Step 42, x: [3.2084527015686035, -4.450076580047607], Loss: 0.4082644283771515\n",
      "Step 43, x: [3.1937026977539062, -4.497252941131592], Loss: 0.3458682894706726\n",
      "Step 44, x: [3.177929401397705, -4.542349815368652], Loss: 0.29027533531188965\n",
      "Step 45, x: [3.1613972187042236, -4.585376262664795], Loss: 0.24110256135463715\n",
      "Step 46, x: [3.144360303878784, -4.626345634460449], Loss: 0.19796191155910492\n",
      "Step 47, x: [3.1270594596862793, -4.6652750968933105], Loss: 0.16045749187469482\n",
      "Step 48, x: [3.1097218990325928, -4.702186584472656], Loss: 0.12818486988544464\n",
      "Step 49, x: [3.0925586223602295, -4.737105369567871], Loss: 0.10073172301054001\n",
      "Step 50, x: [3.075763702392578, -4.7700605392456055], Loss: 0.07768069207668304\n",
      "Step 51, x: [3.0595130920410156, -4.801084995269775], Loss: 0.05861229449510574\n",
      "Step 52, x: [3.043963670730591, -4.830214977264404], Loss: 0.0431089885532856\n",
      "Step 53, x: [3.029252529144287, -4.857490062713623], Loss: 0.03075975738465786\n",
      "Step 54, x: [3.0154967308044434, -4.882952690124512], Loss: 0.0211647916585207\n",
      "Step 55, x: [3.002793312072754, -4.906648635864258], Loss: 0.013940221630036831\n",
      "Step 56, x: [2.9912190437316895, -4.92862606048584], Loss: 0.008722280152142048\n",
      "Step 57, x: [2.9808311462402344, -4.948935508728027], Loss: 0.0051713441498577595\n",
      "Step 58, x: [2.971667528152466, -4.967629432678223], Loss: 0.002975027309730649\n",
      "Step 59, x: [2.963747262954712, -4.984762191772461], Loss: 0.0018505826592445374\n",
      "Step 60, x: [2.9570722579956055, -5.000390529632568], Loss: 0.001546451821923256\n",
      "Step 61, x: [2.9516279697418213, -5.014571666717529], Loss: 0.0018429434858262539\n",
      "Step 62, x: [2.9473845958709717, -5.027364730834961], Loss: 0.002552186604589224\n",
      "Step 63, x: [2.94429874420166, -5.0388288497924805], Loss: 0.003517209319397807\n",
      "Step 64, x: [2.942314624786377, -5.049025058746338], Loss: 0.00461030937731266\n",
      "Step 65, x: [2.9413657188415527, -5.058014392852783], Loss: 0.005731059238314629\n",
      "Step 66, x: [2.9413766860961914, -5.065857410430908], Loss: 0.006803648546338081\n",
      "Step 67, x: [2.9422647953033447, -5.072615623474121], Loss: 0.007773891557008028\n",
      "Step 68, x: [2.943941354751587, -5.078350067138672], Loss: 0.0086063826456666\n",
      "Step 69, x: [2.9463138580322266, -5.083120822906494], Loss: 0.00928130466490984\n",
      "Step 70, x: [2.949287176132202, -5.08698844909668], Loss: 0.009791272692382336\n",
      "Step 71, x: [2.9527649879455566, -5.0900115966796875], Loss: 0.010138780809938908\n",
      "Step 72, x: [2.956651449203491, -5.092248916625977], Loss: 0.010333233512938023\n",
      "Step 73, x: [2.9608523845672607, -5.093757152557373], Loss: 0.01038895919919014\n",
      "Step 74, x: [2.9652767181396484, -5.094592571258545], Loss: 0.010322939604520798\n",
      "Step 75, x: [2.969837188720703, -5.094809532165527], Loss: 0.010153460316359997\n",
      "Step 76, x: [2.9744515419006348, -5.094461441040039], Loss: 0.00989864207804203\n",
      "Step 77, x: [2.979043483734131, -5.093599319458008], Loss: 0.00957568734884262\n",
      "Step 78, x: [2.9835431575775146, -5.092272758483887], Loss: 0.009200007654726505\n",
      "Step 79, x: [2.9878878593444824, -5.090529918670654], Loss: 0.00878508947789669\n",
      "Step 80, x: [2.9920222759246826, -5.088416576385498], Loss: 0.008342370390892029\n",
      "Step 81, x: [2.995899200439453, -5.085977077484131], Loss: 0.007881134748458862\n",
      "Step 82, x: [2.999478816986084, -5.083253860473633], Loss: 0.007408874109387398\n",
      "Step 83, x: [3.0027294158935547, -5.080286502838135], Loss: 0.0069314767606556416\n",
      "Step 84, x: [3.005627393722534, -5.077113151550293], Loss: 0.006453372072428465\n",
      "Step 85, x: [3.0081562995910645, -5.073770046234131], Loss: 0.005978106055408716\n",
      "Step 86, x: [3.0103073120117188, -5.070290565490723], Loss: 0.005508544854819775\n",
      "Step 87, x: [3.012078285217285, -5.066707134246826], Loss: 0.0050470042042434216\n",
      "Step 88, x: [3.0134735107421875, -5.063048839569092], Loss: 0.004595726728439331\n",
      "Step 89, x: [3.014503002166748, -5.0593438148498535], Loss: 0.004156691953539848\n",
      "Step 90, x: [3.0151820182800293, -5.055617332458496], Loss: 0.0037320253904908895\n",
      "Step 91, x: [3.0155303478240967, -5.05189323425293], Loss: 0.003323781304061413\n",
      "Step 92, x: [3.0155715942382812, -5.048192977905273], Loss: 0.002934099640697241\n",
      "Step 93, x: [3.0153326988220215, -5.044536113739014], Loss: 0.002565037691965699\n",
      "Step 94, x: [3.0148427486419678, -5.040940761566162], Loss: 0.0022185570560395718\n",
      "Step 95, x: [3.0141332149505615, -5.0374226570129395], Loss: 0.0018964530900120735\n",
      "Step 96, x: [3.0132360458374023, -5.033996105194092], Loss: 0.0016002029879018664\n",
      "Step 97, x: [3.0121841430664062, -5.030673503875732], Loss: 0.0013309281785041094\n",
      "Step 98, x: [3.01101016998291, -5.0274658203125], Loss: 0.0010893172584474087\n",
      "Step 99, x: [3.0097460746765137, -5.0243821144104], Loss: 0.000875595142133534\n",
      "Step 100, x: [3.0084228515625, -5.021430492401123], Loss: 0.0006894735270179808\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Define a parameter to optimize\n",
    "x = torch.tensor([1.0, -1.0], requires_grad=True)  # Initial values\n",
    "\n",
    "# Initialize the Adam optimizer\n",
    "optimizer = optim.Adam([x], lr=0.1)  # Learning rate is 0.1\n",
    "\n",
    "def simple_loss(x):\n",
    "    return (x[0] - 3)**2 + (x[1] + 5)**2\n",
    "\n",
    "# Optimization loop\n",
    "for step in range(100):\n",
    "    optimizer.zero_grad()  # Reset the gradients to zero\n",
    "\n",
    "    loss = simple_loss(x)  # Compute the current loss\n",
    "\n",
    "    loss.backward()        # Perform backpropagation to compute the gradients\n",
    "\n",
    "    optimizer.step()       # Update parameters using the computed gradients\n",
    "\n",
    "    print(f\"Step {step+1}, x: {x.tolist()}, Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_DFT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
