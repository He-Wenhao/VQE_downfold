Step 1, Loss: -2.1481867748897563  gradient: 0.22052605795533753
Step 11, Loss: -2.065477363961256  gradient: 3.643472419328218
Step 21, Loss: -2.12689608916439  gradient: 1.1753347355542667
Step 31, Loss: -2.146887273055549  gradient: 0.7161277789962393
Step 41, Loss: -2.156458064775878  gradient: 0.40457649521382394
Step 51, Loss: -2.163286734661589  gradient: 0.26922531733328064
Step 61, Loss: -2.167464269315949  gradient: 0.1468969245072927
Step 71, Loss: -2.170370440024697  gradient: 0.08290205902467718
Step 81, Loss: -2.172764436783765  gradient: 0.05995013980829984
Step 91, Loss: -2.174757760565571  gradient: 0.041418676428936235
Step 101, Loss: -2.1762167815425344  gradient: 0.02638937743107097
Step 111, Loss: -2.1772178273238327  gradient: 0.02202904321291089
Step 121, Loss: -2.1779257744755296  gradient: 0.015097701146539647
Step 131, Loss: -2.1784331217175326  gradient: 0.012010964152423
Step 141, Loss: -2.1787823445798686  gradient: 0.007983926699068705
Step 151, Loss: -2.179004885077686  gradient: 0.005986304196937749
Step 161, Loss: -2.179133057113876  gradient: 0.0038323684831426397
Step 171, Loss: -2.179199131518948  gradient: 0.0028253620313967386
Step 181, Loss: -2.179229732037701  gradient: 0.0016068430544756034
Step 191, Loss: -2.179242631413668  gradient: 0.0010666016586733675
convergent at epoch 193; gradient 0.0009465023039728352 is below threshold