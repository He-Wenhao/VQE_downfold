Step 1, Loss: -2.1247946424416035  gradient: 0.19641050719713932
Step 11, Loss: -2.074543201404397  gradient: 3.039292745791149
Step 21, Loss: -2.113680259806479  gradient: 1.101615471915581
Step 31, Loss: -2.1213118738757806  gradient: 0.7283546237005702
Step 41, Loss: -2.130026717274328  gradient: 0.35365521208458583
Step 51, Loss: -2.1353158093365137  gradient: 0.2320716528907476
Step 61, Loss: -2.137846481596291  gradient: 0.1308095446597464
Step 71, Loss: -2.139022096935365  gradient: 0.09368702436292306
Step 81, Loss: -2.139611224314518  gradient: 0.057737989582061444
Step 91, Loss: -2.1399669473525202  gradient: 0.03217870687069173
Step 101, Loss: -2.140346134002187  gradient: 0.026969192475103845
Step 111, Loss: -2.1407824267568265  gradient: 0.021918538235971792
Step 121, Loss: -2.1413569296239716  gradient: 0.01957892426753525
Step 131, Loss: -2.1421673224551423  gradient: 0.022011309472605482
Step 141, Loss: -2.143338548658816  gradient: 0.025848215059121705
Step 151, Loss: -2.1450181837129256  gradient: 0.029972698371328894
Step 161, Loss: -2.1472993273362064  gradient: 0.03358918651991105
Step 171, Loss: -2.1499897901822593  gradient: 0.033950585638490516
Step 181, Loss: -2.152439522287827  gradient: 0.028017717519188308
Step 191, Loss: -2.1540258768602327  gradient: 0.018750663828723844
Step 201, Loss: -2.1547843385840113  gradient: 0.011021281365109168
Step 211, Loss: -2.1550977521681474  gradient: 0.006791321733930811
Step 221, Loss: -2.1552306929938  gradient: 0.004629957567880778
Step 231, Loss: -2.1552918744568683  gradient: 0.003038465991747694
Step 241, Loss: -2.155321201911214  gradient: 0.0019181827826347635
Step 251, Loss: -2.1553353916858398  gradient: 0.0012765694545170219
convergent at epoch 258; gradient 0.0009712195627821698 is below threshold