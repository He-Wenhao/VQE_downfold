Step 1, Loss: -0.9022157718096397  gradient: 1.505854755659044
Step 11, Loss: -0.9011550527893751  gradient: 1.4661722304091565
Step 21, Loss: -0.9258252297007397  gradient: 0.5449321538629834
Step 31, Loss: -0.9316398120363361  gradient: 0.31197541355095776
Step 41, Loss: -0.9355252370811677  gradient: 0.20493272750340616
Step 51, Loss: -0.9379778390666246  gradient: 0.13358862844367558
Step 61, Loss: -0.9387166155657899  gradient: 0.07557045099662792
Step 71, Loss: -0.938870728107766  gradient: 0.0439114774791556
Step 81, Loss: -0.9389601636391951  gradient: 0.0281822584006256
Step 91, Loss: -0.938998438592293  gradient: 0.016928688208619848
Step 101, Loss: -0.9390187128731295  gradient: 0.007867868585287013
Step 111, Loss: -0.9390356436868831  gradient: 0.006432294704288086
Step 121, Loss: -0.9390531048777638  gradient: 0.004179760941510946
Step 131, Loss: -0.9390723629223512  gradient: 0.0030176486800268148
Step 141, Loss: -0.9390940366582635  gradient: 0.002769555360794859
Step 151, Loss: -0.9391187596388799  gradient: 0.0028831307213658557
Step 161, Loss: -0.9391471543627288  gradient: 0.0027428878807501814
Step 171, Loss: -0.9391798309825742  gradient: 0.0029710438458968908
Step 181, Loss: -0.9392175286818923  gradient: 0.0031804500462785153
Step 191, Loss: -0.9392610808419979  gradient: 0.0034499358941524087
Step 201, Loss: -0.9393114356799598  gradient: 0.003719004741269151
Step 211, Loss: -0.9393696412971684  gradient: 0.004020625779860515
Step 221, Loss: -0.9394368145953688  gradient: 0.004354400051897965
Step 231, Loss: -0.9395140800908811  gradient: 0.004695803435011886
Step 241, Loss: -0.9396024673333939  gradient: 0.005040613773735876
Step 251, Loss: -0.9397027590186253  gradient: 0.005382627010340667
Step 261, Loss: -0.9398152892678232  gradient: 0.0056979419206533806
Step 271, Loss: -0.9399397087279907  gradient: 0.0059665030624744975
Step 281, Loss: -0.9400747568285596  gradient: 0.006165418400134463
Step 291, Loss: -0.9402181074811253  gradient: 0.006269560177573837
Step 301, Loss: -0.9403663676284391  gradient: 0.006260229615827703
Step 311, Loss: -0.9405152903878073  gradient: 0.006126694634761412
Step 321, Loss: -0.9406602080152422  gradient: 0.005870105518011399
Step 331, Loss: -0.9407966096384538  gradient: 0.0055122401390167
Step 341, Loss: -0.9409207229000065  gradient: 0.005080998137717906
Step 351, Loss: -0.9410299470209477  gradient: 0.0045908685367879
Step 361, Loss: -0.9411230378408086  gradient: 0.004104994469738457
Step 371, Loss: -0.9412000341517969  gradient: 0.0036560420749231115
Step 381, Loss: -0.9412619927187678  gradient: 0.003212025537045093
Step 391, Loss: -0.941310635129162  gradient: 0.002792727958682024
Step 401, Loss: -0.9413480010926624  gradient: 0.0024083388190467014
Step 411, Loss: -0.941376168849174  gradient: 0.0020627948630996132
Step 421, Loss: -0.9413970658028848  gradient: 0.001751743066951654
Step 431, Loss: -0.9414123648208956  gradient: 0.0014775638352942703
Step 441, Loss: -0.9414234473143002  gradient: 0.0012395099549203882
Step 451, Loss: -0.9414314106495342  gradient: 0.0010356195676865959
convergent at epoch 453; gradient 0.0009991779825224297 is below threshold