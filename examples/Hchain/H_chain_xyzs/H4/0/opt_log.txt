Step 1, Loss: -2.172955502950544  gradient: 0.19350546032956922
Step 11, Loss: -2.1132007137112065  gradient: 3.1456275464250414
Step 21, Loss: -2.1593330825420085  gradient: 1.084736816178923
Step 31, Loss: -2.169221240283653  gradient: 0.533491283582957
Step 41, Loss: -2.1733253092078195  gradient: 0.33570628242952716
Step 51, Loss: -2.175874867861436  gradient: 0.22900143721209426
Step 61, Loss: -2.1781599823317768  gradient: 0.1403669892483704
Step 71, Loss: -2.180326984684238  gradient: 0.08779832591229539
Step 81, Loss: -2.1825459061441266  gradient: 0.06601286999452602
Step 91, Loss: -2.184805467092987  gradient: 0.05090549259292214
Step 101, Loss: -2.187027741091436  gradient: 0.04347238304634765
Step 111, Loss: -2.189271997562782  gradient: 0.03941688622476453
Step 121, Loss: -2.1918150023617584  gradient: 0.03981027876015824
Step 131, Loss: -2.194877309714356  gradient: 0.039889898257520366
Step 141, Loss: -2.198118566293106  gradient: 0.03394202506873203
Step 151, Loss: -2.20074109372773  gradient: 0.023449824256241855
Step 161, Loss: -2.202421655736669  gradient: 0.01669434340752671
Step 171, Loss: -2.203308195632988  gradient: 0.01129213919266234
Step 181, Loss: -2.203683811497834  gradient: 0.007081111461688304
Step 191, Loss: -2.2038394673006425  gradient: 0.0045682235339124175
Step 201, Loss: -2.2039117629648386  gradient: 0.002751096917184186
Step 211, Loss: -2.203941924850543  gradient: 0.001457124265298942
convergent at epoch 218; gradient 0.0009840705868299267 is below threshold