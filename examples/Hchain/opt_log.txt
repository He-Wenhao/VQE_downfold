Step 1, Loss: -0.834356065343443  gradient: 1.6704446763905207
Step 11, Loss: -0.8373366990824969  gradient: 1.63709388371689
Step 21, Loss: -0.8612678483997926  gradient: 0.5439670075057407
Step 31, Loss: -0.8663825847600725  gradient: 0.32199840201347385
Step 41, Loss: -0.8675249168920325  gradient: 0.18776762471888991
Step 51, Loss: -0.8681060426300473  gradient: 0.11773570240233266
Step 61, Loss: -0.8684502435679853  gradient: 0.07382342554099325
Step 71, Loss: -0.8688568071950659  gradient: 0.04411349575604213
Step 81, Loss: -0.8694086217436805  gradient: 0.030609818207194387
Step 91, Loss: -0.8702362802992691  gradient: 0.026126884384565455
Step 101, Loss: -0.8710629402144807  gradient: 0.01670134424566312
Step 111, Loss: -0.8716021370157372  gradient: 0.013464436702657604
Step 121, Loss: -0.8721231881747392  gradient: 0.01303725618478286
Step 131, Loss: -0.8726888135161099  gradient: 0.010265864322865843
Step 141, Loss: -0.8730731381864163  gradient: 0.006771404367309991
Step 151, Loss: -0.8732732236977123  gradient: 0.005384449101389812
Step 161, Loss: -0.8733801520567983  gradient: 0.004724738256177956
Step 171, Loss: -0.8734612518329483  gradient: 0.004907428314107265
Step 181, Loss: -0.8735460319091672  gradient: 0.00528850443875309
Step 191, Loss: -0.87364287621287  gradient: 0.005447774808676441
Step 201, Loss: -0.8737512195497813  gradient: 0.005676666627829643
Step 211, Loss: -0.8738692992147216  gradient: 0.005822237694939717
Step 221, Loss: -0.8739954392717064  gradient: 0.005904452748662634
Step 231, Loss: -0.874127004053238  gradient: 0.005888111988918048
Step 241, Loss: -0.874260338869753  gradient: 0.005781472897055563
Step 251, Loss: -0.8743912773600918  gradient: 0.005575844323710361
Step 261, Loss: -0.8745157341825948  gradient: 0.005275120381909349
Step 271, Loss: -0.8746302354040962  gradient: 0.004901516371101862
Step 281, Loss: -0.8747323102984912  gradient: 0.0044811985325429494
Step 291, Loss: -0.874820666343331  gradient: 0.004042220475442618
Step 301, Loss: -0.8748951313761708  gradient: 0.003608339809461365
Step 311, Loss: -0.8749564276410141  gradient: 0.0031959265535563804
Step 321, Loss: -0.875005871786608  gradient: 0.002837293223387958
Step 331, Loss: -0.8750450832345305  gradient: 0.002510041568379867
Step 341, Loss: -0.8750757504904138  gradient: 0.0022082600767745616
Step 351, Loss: -0.8750994721443119  gradient: 0.0019348594895963972
Step 361, Loss: -0.875117667030314  gradient: 0.0016925560024447055
Step 371, Loss: -0.8751315375545925  gradient: 0.0014748075387212962
Step 381, Loss: -0.8751420683466989  gradient: 0.001281724724484349
Step 391, Loss: -0.8751500450668053  gradient: 0.001110531827711773
convergent at epoch 399; gradient 0.000989905001498848 is below threshold